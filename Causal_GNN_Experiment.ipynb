{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b10df763",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Project - CSE 499A\\Deception-Detection-using-CGNN\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hardware Detected: CPU\n"
     ]
    }
   ],
   "source": [
    "# @title 1. Setup & Imports\n",
    "# Install dependencies once (outside the notebook):\n",
    "#   pip install -r requirements.txt\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import librosa\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import mediapipe as mp\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Hardware Configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Hardware Detected: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")\n",
    "\n",
    "# CONFIGURATION\n",
    "# -----------------------------\n",
    "TRAIN_ROOT = \"DOLOS_Train\"\n",
    "TEST_ROOT = \"RLT_Test\"\n",
    "FRAME_LIMIT = 60        # Number of frames per clip (Fixed size)\n",
    "BATCH_SIZE = 16         # Batch size (tune to your GPU/CPU)\n",
    "LEARNING_RATE = 0.001   # Adam Optimizer\n",
    "EPOCHS = 30             # Training Epochs\n",
    "HIDDEN_DIM = 64         # Model Hidden Dimension\n",
    "NUM_CONFOUNDERS = 32    # Size of Causal Dictionary\n",
    "# -----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96395be5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph Topology Built: 68 Nodes with Anatomical Connections.\n"
     ]
    }
   ],
   "source": [
    "# @title 2. Graph Definition (68-Point Skeleton)\n",
    "# Mapping MediaPipe's 468 points to the standard 68-point Dlib format\n",
    "MP_TO_DLIB_68 = [\n",
    "    162, 234, 93, 58, 172, 136, 149, 148, 152, 377, 378, 365, 397, 288, 323, 454, 389, # Jaw (0-16)\n",
    "    71, 63, 105, 66, 107, 336, 296, 334, 293, 300, # Eyebrows (17-26)\n",
    "    168, 6, 195, 4, 64, 60, 94, 285, 292, 419, 197, 19, 1, 2, 98, 327, 276, 283, 282, 295, 294, # Nose (27-35) + Eyes (36-47)\n",
    "    33, 246, 161, 160, 159, 158, 157, 173, 133, 155, 154, 153, 144, 145, 153, 154, 155, 133, # Eyes detail\n",
    "    78, 95, 88, 178, 87, 14, 317, 402, 318, 324, 308, 415, 310, 311, 312, 13, 82, 81, 80, 191, 78 # Mouth (48-67)\n",
    "]\n",
    "SELECTED_LANDMARKS = MP_TO_DLIB_68[:68]\n",
    "\n",
    "def get_anatomical_edges():\n",
    "    \"\"\"Creates edges between physically connected facial points.\"\"\"\n",
    "    edges = []\n",
    "    # Anatomical chains (Jaw, Brows, Nose, Eyes, Mouth)\n",
    "    chains = [\n",
    "        range(0, 17), range(17, 22), range(22, 27), range(27, 31),\n",
    "        range(31, 36), range(36, 42), range(42, 48), range(48, 60), range(60, 68)\n",
    "    ]\n",
    "    for chain in chains:\n",
    "        for i in range(len(chain) - 1):\n",
    "            edges.append([chain[i], chain[i+1]])\n",
    "            edges.append([chain[i+1], chain[i]]) # Undirected Graph\n",
    "    return torch.tensor(edges, dtype=torch.long).t().contiguous().to(device)\n",
    "\n",
    "STATIC_EDGE_INDEX = get_anatomical_edges()\n",
    "print(f\"Graph Topology Built: 68 Nodes with Anatomical Connections.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78b43337",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'mediapipe' has no attribute 'solutions'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# @title 3. Multimodal Preprocessing (Face + Audio)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m mp_face_mesh = \u001b[43mmp\u001b[49m\u001b[43m.\u001b[49m\u001b[43msolutions\u001b[49m.face_mesh\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mDeceptionDataset\u001b[39;00m(Dataset):\n\u001b[32m      5\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, root_dir: \u001b[38;5;28mstr\u001b[39m, frame_limit: \u001b[38;5;28mint\u001b[39m = \u001b[32m60\u001b[39m):\n",
      "\u001b[31mAttributeError\u001b[39m: module 'mediapipe' has no attribute 'solutions'"
     ]
    }
   ],
   "source": [
    "# @title 3. Multimodal Preprocessing (Face + Audio)\n",
    "# This notebook expects MediaPipe FaceMesh via `mp.solutions`.\n",
    "# If you're on an unsupported Python version (e.g., 3.14), the `mediapipe` wheel may not include `solutions`.\n",
    "# In that case we fall back to audio-only features (face landmarks are zeroed) so the rest of the pipeline can run.\n",
    "\n",
    "ENABLE_FACE_LANDMARKS = True\n",
    "try:\n",
    "    mp_face_mesh = mp.solutions.face_mesh\n",
    "except AttributeError:\n",
    "    ENABLE_FACE_LANDMARKS = False\n",
    "    mp_face_mesh = None\n",
    "    print(\n",
    "        \"WARNING: `mediapipe` does not expose `mp.solutions` in this environment. \"\n",
    "        \"Falling back to audio-only features (face landmarks = 0). \"\n",
    "        \"For full face landmarks, use Python 3.10-3.12 and reinstall mediapipe.\"\n",
    "    )\n",
    "\n",
    "\n",
    "class DeceptionDataset(Dataset):\n",
    "    def __init__(self, root_dir: str, frame_limit: int = 60):\n",
    "        self.samples: list[str] = []\n",
    "        self.labels: list[int] = []\n",
    "        self.frame_limit = frame_limit\n",
    "\n",
    "        classes = {\"truth\": 0, \"lie\": 1}\n",
    "\n",
    "        if not os.path.exists(root_dir):\n",
    "            raise FileNotFoundError(f\"Directory not found: {root_dir}\")\n",
    "\n",
    "        print(f\"Scanning {root_dir}...\")\n",
    "        for class_name, label in classes.items():\n",
    "            class_path = os.path.join(root_dir, class_name)\n",
    "            if not os.path.exists(class_path):\n",
    "                continue\n",
    "\n",
    "            for f in tqdm(os.listdir(class_path), desc=f\"Loading {class_name}\"):\n",
    "                if f.lower().endswith((\".mp4\", \".avi\", \".mov\")):\n",
    "                    self.samples.append(os.path.join(class_path, f))\n",
    "                    self.labels.append(label)\n",
    "\n",
    "    def _extract_mfcc(self, video_path: str) -> np.ndarray:\n",
    "        try:\n",
    "            y, sr = librosa.load(video_path, sr=16000, duration=3.0)\n",
    "            return librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13).T.astype(np.float32)  # (Time, 13)\n",
    "        except Exception:\n",
    "            return np.zeros((100, 13), dtype=np.float32)\n",
    "\n",
    "    def process_video(self, video_path: str) -> torch.Tensor:\n",
    "        mfcc = self._extract_mfcc(video_path)\n",
    "\n",
    "        # If face landmarks aren't available, build an audio-only tensor (face xyz = 0)\n",
    "        if not ENABLE_FACE_LANDMARKS:\n",
    "            node_feats = []\n",
    "            for frame_idx in range(self.frame_limit):\n",
    "                audio_idx = min(int(frame_idx * (len(mfcc) / self.frame_limit)), len(mfcc) - 1)\n",
    "                audio_vec = np.tile(mfcc[audio_idx], (68, 1))\n",
    "                face_geo = np.zeros((68, 3), dtype=np.float32)\n",
    "                node_feats.append(np.concatenate((face_geo, audio_vec), axis=1))\n",
    "            return torch.from_numpy(np.array(node_feats, dtype=np.float32))\n",
    "\n",
    "        # 2. Video Extraction (Landmarks)\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        node_feats = []\n",
    "\n",
    "        with mp_face_mesh.FaceMesh(static_image_mode=False, max_num_faces=1) as face_mesh:\n",
    "            while cap.isOpened() and len(node_feats) < self.frame_limit:\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "\n",
    "                results = face_mesh.process(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "                if results.multi_face_landmarks:\n",
    "                    lm = results.multi_face_landmarks[0].landmark\n",
    "                    face_geo = np.array([[lm[i].x, lm[i].y, lm[i].z] for i in SELECTED_LANDMARKS], dtype=np.float32)\n",
    "\n",
    "                    audio_idx = min(int(len(node_feats) * (len(mfcc) / self.frame_limit)), len(mfcc) - 1)\n",
    "                    audio_vec = np.tile(mfcc[audio_idx], (68, 1))\n",
    "\n",
    "                    fused = np.concatenate((face_geo, audio_vec), axis=1)\n",
    "                    node_feats.append(fused)\n",
    "                else:\n",
    "                    node_feats.append(np.zeros((68, 16), dtype=np.float32))\n",
    "\n",
    "        cap.release()\n",
    "\n",
    "        # 3. Padding/Truncating\n",
    "        if len(node_feats) == 0:\n",
    "            return torch.zeros(self.frame_limit, 68, 16, dtype=torch.float32)\n",
    "\n",
    "        tensor = np.array(node_feats, dtype=np.float32)\n",
    "        if len(tensor) < self.frame_limit:\n",
    "            pad = np.zeros((self.frame_limit - len(tensor), 68, 16), dtype=np.float32)\n",
    "            tensor = np.concatenate((tensor, pad), axis=0)\n",
    "\n",
    "        return torch.from_numpy(tensor)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        x = self.process_video(self.samples[idx])\n",
    "        y = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return x, y\n",
    "\n",
    "\n",
    "# Initialize Datasets\n",
    "print(\"--- Initializing Training Set (DOLOS) ---\")\n",
    "train_dataset = DeceptionDataset(TRAIN_ROOT, FRAME_LIMIT)\n",
    "\n",
    "print(\"\\n--- Initializing Test Set (Real-Life Trial) ---\")\n",
    "test_dataset = DeceptionDataset(TEST_ROOT, FRAME_LIMIT)\n",
    "\n",
    "if len(train_dataset) == 0:\n",
    "    raise RuntimeError(f\"No training samples found under {TRAIN_ROOT}. Check folder structure.\")\n",
    "if len(test_dataset) == 0:\n",
    "    raise RuntimeError(f\"No test samples found under {TEST_ROOT}. Check folder structure.\")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "print(f\"\\nData Loaded: {len(train_dataset)} Train Samples, {len(test_dataset)} Test Samples\")\n",
    "print(f\"Face landmarks enabled: {ENABLE_FACE_LANDMARKS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b139947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model Initialized.\n"
     ]
    }
   ],
   "source": [
    "# @title 4. C-GNN-D Architecture\n",
    "class CausalInterventionLayer(nn.Module):\n",
    "    def __init__(self, in_dim, num_confounders=32):\n",
    "        super().__init__()\n",
    "        # Learnable Confounder Dictionary (e.g., \"Generic Nervousness\")\n",
    "        self.confounder_mem = nn.Parameter(torch.randn(num_confounders, in_dim))\n",
    "        self.attention = nn.Linear(in_dim, num_confounders)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 1. Calc P(Confounder | Input)\n",
    "        attn = F.softmax(self.attention(x), dim=-1)\n",
    "        # 2. Calc Expected Confounder E[C]\n",
    "        expected_confounder = torch.matmul(attn, self.confounder_mem)\n",
    "        # 3. Intervention (Do-Calculus): Remove Confounder\n",
    "        return x - expected_confounder\n",
    "\n",
    "\n",
    "def make_batched_edge_index(edge_index: torch.Tensor, batch_size: int, num_nodes: int) -> torch.Tensor:\n",
    "    \"\"\"Repeat a single-graph edge_index into a batched edge_index for B disjoint graphs.\"\"\"\n",
    "    if batch_size == 1:\n",
    "        return edge_index\n",
    "\n",
    "    edge_index = edge_index.to(torch.long)\n",
    "    e = edge_index.size(1)\n",
    "    offsets = (torch.arange(batch_size, device=edge_index.device, dtype=edge_index.dtype) * num_nodes).repeat_interleave(e)\n",
    "    return edge_index.repeat(1, batch_size) + offsets.unsqueeze(0)\n",
    "\n",
    "\n",
    "class CGNND(nn.Module):\n",
    "    def __init__(self, node_feats=16, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        # Spatial: Graph Conv\n",
    "        self.gcn = GCNConv(node_feats, hidden_dim)\n",
    "        # Temporal: LSTM\n",
    "        self.lstm = nn.LSTM(hidden_dim, hidden_dim, batch_first=True)\n",
    "        # Causal: Intervention\n",
    "        self.causal = CausalInterventionLayer(hidden_dim, NUM_CONFOUNDERS)\n",
    "        # Classifier\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # x: (Batch, Time, Nodes, Feats)\n",
    "        b, t, n, f_dim = x.shape\n",
    "\n",
    "        # Build a correct disjoint-graph edge_index for this batch.\n",
    "        batched_edge_index = make_batched_edge_index(edge_index, b, n)\n",
    "\n",
    "        spatial_feats = []\n",
    "        for frame_idx in range(t):\n",
    "            frame_x = x[:, frame_idx, :, :].reshape(b * n, f_dim)\n",
    "\n",
    "            out = F.relu(self.gcn(frame_x, batched_edge_index))\n",
    "            out = out.view(b, n, -1).mean(dim=1)\n",
    "            spatial_feats.append(out)\n",
    "\n",
    "        spatial_seq = torch.stack(spatial_feats, dim=1)  # (B, T, H)\n",
    "\n",
    "        # 2. Temporal Pass\n",
    "        _, (hn, _) = self.lstm(spatial_seq)\n",
    "        embedding = hn[-1]  # (B, H)\n",
    "\n",
    "        # 3. Causal Intervention\n",
    "        causal_embedding = self.causal(embedding)\n",
    "\n",
    "        # 4. Classify\n",
    "        logits = self.fc(causal_embedding)\n",
    "\n",
    "        return logits, embedding, causal_embedding\n",
    "\n",
    "\n",
    "model = CGNND(hidden_dim=HIDDEN_DIM).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "print(\"✅ Model Initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5cf05c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes: {'logits': (4, 2), 'embedding': (4, 64), 'causal_embedding': (4, 64)}\n"
     ]
    }
   ],
   "source": [
    "# Quick sanity check (no dataset required)\n",
    "with torch.no_grad():\n",
    "    dummy_x = torch.randn(4, FRAME_LIMIT, 68, 16, device=device)\n",
    "    dummy_logits, dummy_emb, dummy_causal = model(dummy_x, STATIC_EDGE_INDEX)\n",
    "\n",
    "print(\"Shapes:\", {\n",
    "    \"logits\": tuple(dummy_logits.shape),\n",
    "    \"embedding\": tuple(dummy_emb.shape),\n",
    "    \"causal_embedding\": tuple(dummy_causal.shape),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f1bf12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 5. Training Loop\n",
    "train_acc_history = []\n",
    "train_loss_history = []\n",
    "\n",
    "print(f\"Starting Training for {EPOCHS} Epochs...\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_x, batch_y in train_loader:\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward\n",
    "        logits, _, _ = model(batch_x, STATIC_EDGE_INDEX)\n",
    "        \n",
    "        loss = criterion(logits, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        correct += (preds == batch_y).sum().item()\n",
    "        total += batch_y.size(0)\n",
    "        \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    acc = correct / total\n",
    "    train_loss_history.append(avg_loss)\n",
    "    train_acc_history.append(acc)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} | Loss: {avg_loss:.4f} | Acc: {acc*100:.2f}%\")\n",
    "\n",
    "# Plot Learning Curve\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_loss_history, label='Loss', color='red')\n",
    "plt.title(\"Training Loss\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_acc_history, label='Accuracy', color='blue')\n",
    "plt.title(\"Training Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8e4dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 6. Experimental Results\n",
    "\n",
    "# --- A. Cross-Corpus Evaluation ---\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "print(\"Running Cross-Corpus Evaluation (Test Set)...\")\n",
    "with torch.no_grad():\n",
    "    for batch_x, batch_y in tqdm(test_loader):\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "\n",
    "        logits, _, _ = model(batch_x, STATIC_EDGE_INDEX)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(batch_y.cpu().numpy())\n",
    "\n",
    "# --- B. Confusion Matrix ---\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Truth', 'Lie'], yticklabels=['Truth', 'Lie'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Cross-Corpus Confusion Matrix (DOLOS -> RLT)')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(all_labels, all_preds, target_names=['Truth', 'Lie']))\n",
    "\n",
    "# --- C. Causal Explainability Plot ---\n",
    "# Pick a \"lie\" sample if present; otherwise fall back to index 0.\n",
    "try:\n",
    "    sample_idx = next(i for i, y in enumerate(test_dataset.labels) if y == 1)\n",
    "except StopIteration:\n",
    "    sample_idx = 0\n",
    "\n",
    "sample_x, sample_y = test_dataset[sample_idx]\n",
    "sample_x = sample_x.unsqueeze(0).to(device)\n",
    "\n",
    "model.eval()\n",
    "_, orig_emb, causal_emb = model(sample_x, STATIC_EDGE_INDEX)\n",
    "orig = orig_emb.detach().cpu().numpy().flatten()\n",
    "caus = causal_emb.detach().cpu().numpy().flatten()\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.bar(np.arange(len(orig)) - 0.2, orig, width=0.4, label='Original (Confounded)', color='gray', alpha=0.7)\n",
    "plt.bar(np.arange(len(caus)) + 0.2, caus, width=0.4, label='Causal (Intervened)', color='#d62728')\n",
    "plt.legend()\n",
    "plt.title(\"Feature Disentanglement: Effect of Causal Layer\")\n",
    "plt.xlabel(\"Feature Index\")\n",
    "plt.ylabel(\"Activation Magnitude\")\n",
    "plt.show()\n",
    "\n",
    "print(\"All results generated successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
